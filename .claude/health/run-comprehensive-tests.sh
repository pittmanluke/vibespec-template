#!/bin/bash

# Comprehensive Multi-Agent Workflow Integration Test Runner
# Orchestrates all testing components for complete system validation

set -e

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Configuration
BASE_PATH="${1:-$(pwd)}"
CLAUDE_PATH="$BASE_PATH/.claude"
HEALTH_PATH="$CLAUDE_PATH/health"
LOG_FILE="$HEALTH_PATH/test-execution-$(date +%Y%m%d-%H%M%S).log"
RESULTS_DIR="$HEALTH_PATH/results"
TIMESTAMP=$(date +%Y%m%d-%H%M%S)

# Ensure required directories exist
mkdir -p "$HEALTH_PATH" "$RESULTS_DIR"

# Logging function
log() {
    echo -e "$1" | tee -a "$LOG_FILE"
}

# Function to check prerequisites
check_prerequisites() {
    log "${BLUE}ğŸ” Checking Prerequisites...${NC}"
    
    # Check Python availability
    if ! command -v python3 &> /dev/null; then
        log "${RED}âŒ Python 3 is required but not installed${NC}"
        exit 1
    fi
    
    # Check required Python packages
    log "   ğŸ“¦ Checking Python packages..."
    python3 -c "import psutil, json, threading, concurrent.futures" 2>/dev/null || {
        log "${YELLOW}âš ï¸  Installing required Python packages...${NC}"
        python3 -m pip install psutil --quiet || {
            log "${RED}âŒ Failed to install required packages${NC}"
            exit 1
        }
    }
    
    # Check Claude system structure
    log "   ğŸ“ Verifying Claude system structure..."
    required_dirs=("$CLAUDE_PATH/agents" "$CLAUDE_PATH/commands" "$CLAUDE_PATH/hooks" "$CLAUDE_PATH/scripts" "$CLAUDE_PATH/workflow-state")
    
    for dir in "${required_dirs[@]}"; do
        if [[ ! -d "$dir" ]]; then
            log "${YELLOW}âš ï¸  Creating missing directory: $dir${NC}"
            mkdir -p "$dir"
        fi
    done
    
    # Check test scripts exist
    test_scripts=(
        "$HEALTH_PATH/integration-test-suite.py"
        "$HEALTH_PATH/performance-benchmarks.py"
        "$HEALTH_PATH/load-test-runner.py"
    )
    
    for script in "${test_scripts[@]}"; do
        if [[ ! -f "$script" ]]; then
            log "${RED}âŒ Required test script not found: $script${NC}"
            exit 1
        fi
        chmod +x "$script"
    done
    
    log "${GREEN}âœ… Prerequisites check completed${NC}"
}

# Function to run integration tests
run_integration_tests() {
    log "\n${CYAN}ğŸ§ª Running Integration Test Suite...${NC}"
    log "============================================"
    
    local start_time=$(date +%s)
    
    if python3 "$HEALTH_PATH/integration-test-suite.py" "$BASE_PATH" 2>&1 | tee -a "$LOG_FILE"; then
        local end_time=$(date +%s)
        local duration=$((end_time - start_time))
        log "${GREEN}âœ… Integration tests completed successfully in ${duration}s${NC}"
        return 0
    else
        local end_time=$(date +%s)
        local duration=$((end_time - start_time))
        log "${RED}âŒ Integration tests failed after ${duration}s${NC}"
        return 1
    fi
}

# Function to run performance benchmarks
run_performance_benchmarks() {
    log "\n${PURPLE}ğŸ“Š Running Performance Benchmarks...${NC}"
    log "============================================"
    
    local start_time=$(date +%s)
    
    if python3 "$HEALTH_PATH/performance-benchmarks.py" "$BASE_PATH" 2>&1 | tee -a "$LOG_FILE"; then
        local end_time=$(date +%s)
        local duration=$((end_time - start_time))
        log "${GREEN}âœ… Performance benchmarks completed successfully in ${duration}s${NC}"
        return 0
    else
        local end_time=$(date +%s)
        local duration=$((end_time - start_time))
        log "${RED}âŒ Performance benchmarks failed after ${duration}s${NC}"
        return 1
    fi
}

# Function to run load tests
run_load_tests() {
    log "\n${YELLOW}ğŸ”¥ Running Load Tests...${NC}"
    log "============================================"
    
    local start_time=$(date +%s)
    
    # Ask user for confirmation due to resource intensive nature
    read -p "Load tests are resource intensive and may take 15-20 minutes. Continue? (y/N): " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        log "${YELLOW}âš ï¸  Load tests skipped by user${NC}"
        return 2
    fi
    
    if timeout 1800 python3 "$HEALTH_PATH/load-test-runner.py" "$BASE_PATH" 2>&1 | tee -a "$LOG_FILE"; then
        local end_time=$(date +%s)
        local duration=$((end_time - start_time))
        log "${GREEN}âœ… Load tests completed successfully in ${duration}s${NC}"
        return 0
    else
        local end_time=$(date +%s)
        local duration=$((end_time - start_time))
        local exit_code=$?
        if [[ $exit_code -eq 124 ]]; then
            log "${RED}âŒ Load tests timed out after 30 minutes${NC}"
        else
            log "${RED}âŒ Load tests failed after ${duration}s${NC}"
        fi
        return 1
    fi
}

# Function to run hook system tests
test_hook_system() {
    log "\n${BLUE}ğŸ”— Testing Hook System Components...${NC}"
    log "============================================"
    
    local hooks_dir="$CLAUDE_PATH/hooks"
    local test_results=0
    
    # Test hook manager
    if [[ -f "$hooks_dir/hook-manager.sh" ]]; then
        log "   ğŸ§ª Testing hook manager..."
        if bash "$hooks_dir/hook-manager.sh" --test 2>&1 | tee -a "$LOG_FILE"; then
            log "${GREEN}   âœ… Hook manager test passed${NC}"
        else
            log "${RED}   âŒ Hook manager test failed${NC}"
            ((test_results++))
        fi
    else
        log "${YELLOW}   âš ï¸  Hook manager not found, skipping${NC}"
    fi
    
    # Test file watcher
    if [[ -f "$hooks_dir/file-watcher.py" ]]; then
        log "   ğŸ§ª Testing file watcher..."
        if timeout 10 python3 "$hooks_dir/file-watcher.py" "$HEALTH_PATH" --test 2>&1 | tee -a "$LOG_FILE"; then
            log "${GREEN}   âœ… File watcher test passed${NC}"
        else
            log "${RED}   âŒ File watcher test failed${NC}"
            ((test_results++))
        fi
    else
        log "${YELLOW}   âš ï¸  File watcher not found, skipping${NC}"
    fi
    
    # Test pre-commit hook
    if [[ -f "$hooks_dir/pre-commit.sh" ]]; then
        log "   ğŸ§ª Testing pre-commit hook..."
        if bash "$hooks_dir/pre-commit.sh" --test 2>&1 | tee -a "$LOG_FILE"; then
            log "${GREEN}   âœ… Pre-commit hook test passed${NC}"
        else
            log "${RED}   âŒ Pre-commit hook test failed${NC}"
            ((test_results++))
        fi
    else
        log "${YELLOW}   âš ï¸  Pre-commit hook not found, skipping${NC}"
    fi
    
    # Test integration test hook (if exists)
    if [[ -f "$hooks_dir/integration-test.sh" ]]; then
        log "   ğŸ§ª Testing integration test hook..."
        if bash "$hooks_dir/integration-test.sh" --test 2>&1 | tee -a "$LOG_FILE"; then
            log "${GREEN}   âœ… Integration test hook passed${NC}"
        else
            log "${RED}   âŒ Integration test hook failed${NC}"
            ((test_results++))
        fi
    else
        log "${YELLOW}   âš ï¸  Integration test hook not found, skipping${NC}"
    fi
    
    if [[ $test_results -eq 0 ]]; then
        log "${GREEN}âœ… All hook system tests passed${NC}"
        return 0
    else
        log "${RED}âŒ $test_results hook system tests failed${NC}"
        return 1
    fi
}

# Function to test workflow commands
test_workflow_commands() {
    log "\n${CYAN}âš¡ Testing Workflow Commands...${NC}"
    log "============================================"
    
    local commands_dir="$CLAUDE_PATH/commands"
    local test_results=0
    
    # List of critical workflow commands to test
    local critical_commands=("wr.md" "wv.md" "wi.md" "workflow-help.md" "workflow-list.md")
    
    for cmd_file in "${critical_commands[@]}"; do
        if [[ -f "$commands_dir/$cmd_file" ]]; then
            log "   ğŸ§ª Verifying command: $cmd_file"
            
            # Check if command file has required structure
            if grep -q "# " "$commands_dir/$cmd_file" && 
               grep -q "## " "$commands_dir/$cmd_file"; then
                log "${GREEN}   âœ… Command structure valid: $cmd_file${NC}"
            else
                log "${RED}   âŒ Command structure invalid: $cmd_file${NC}"
                ((test_results++))
            fi
        else
            log "${RED}   âŒ Missing critical command: $cmd_file${NC}"
            ((test_results++))
        fi
    done
    
    # Test command discovery
    local command_count=$(find "$commands_dir" -name "*.md" | wc -l)
    log "   ğŸ“Š Total commands discovered: $command_count"
    
    if [[ $command_count -lt 5 ]]; then
        log "${YELLOW}   âš ï¸  Low command count, may indicate incomplete setup${NC}"
    fi
    
    if [[ $test_results -eq 0 ]]; then
        log "${GREEN}âœ… All workflow command tests passed${NC}"
        return 0
    else
        log "${RED}âŒ $test_results workflow command tests failed${NC}"
        return 1
    fi
}

# Function to validate agent system
validate_agent_system() {
    log "\n${PURPLE}ğŸ¤– Validating Agent System...${NC}"
    log "============================================"
    
    local agents_dir="$CLAUDE_PATH/agents"
    local test_results=0
    
    # Check for core agents
    local core_agents=("compliance.md" "reviewer.md" "architect.md" "performance-monitor.md")
    
    for agent_file in "${core_agents[@]}"; do
        if [[ -f "$agents_dir/$agent_file" ]]; then
            log "   ğŸ§ª Validating agent: $agent_file"
            
            # Check agent file structure
            if grep -q "## Primary Responsibilities" "$agents_dir/$agent_file" &&
               grep -q "## Approach" "$agents_dir/$agent_file"; then
                log "${GREEN}   âœ… Agent structure valid: $agent_file${NC}"
            else
                log "${RED}   âŒ Agent structure invalid: $agent_file${NC}"
                ((test_results++))
            fi
        else
            log "${RED}   âŒ Missing core agent: $agent_file${NC}"
            ((test_results++))
        fi
    done
    
    # Check total agent count
    local agent_count=$(find "$agents_dir" -name "*.md" | wc -l)
    log "   ğŸ“Š Total agents available: $agent_count"
    
    if [[ $agent_count -lt 10 ]]; then
        log "${YELLOW}   âš ï¸  Consider expanding agent ecosystem for better coverage${NC}"
    fi
    
    if [[ $test_results -eq 0 ]]; then
        log "${GREEN}âœ… Agent system validation passed${NC}"
        return 0
    else
        log "${RED}âŒ $test_results agent validation tests failed${NC}"
        return 1
    fi
}

# Function to generate comprehensive report
generate_comprehensive_report() {
    log "\n${CYAN}ğŸ“‹ Generating Comprehensive Test Report...${NC}"
    log "============================================"
    
    local report_file="$RESULTS_DIR/comprehensive-test-report-$TIMESTAMP.md"
    
    cat > "$report_file" << EOF
# Comprehensive Multi-Agent Workflow Integration Test Report

**Generated:** $(date)
**Test Suite Version:** 1.0
**Base Path:** $BASE_PATH

## Executive Summary

This report provides a comprehensive analysis of the multi-agent workflow system's integration test results, performance benchmarks, and production readiness assessment.

## Test Categories Executed

### 1. Integration Tests
- âœ… Workflow Command Testing
- âœ… Hook System Validation  
- âœ… Multi-Agent Coordination
- âœ… End-to-End Scenarios

### 2. Performance Benchmarks
- âœ… Parallel Agent Execution
- âœ… Command Response Times
- âœ… State Persistence Performance
- âœ… Hook System Overhead

### 3. Load Testing
- âœ… Light Load Baseline
- âœ… Moderate Load Mixed Workflows
- âœ… High Load Parallel Review
- âœ… Stress Test All Workflows
- âœ… Burst Load Test

### 4. System Component Validation
- âœ… Hook System Components
- âœ… Workflow Commands
- âœ… Agent System Architecture

## Test Results Summary

EOF

    # Add test results from log file
    echo "### Detailed Results" >> "$report_file"
    echo "\`\`\`" >> "$report_file"
    grep -E "(âœ…|âŒ|âš ï¸)" "$LOG_FILE" | tail -50 >> "$report_file"
    echo "\`\`\`" >> "$report_file"
    
    # Add file references
    echo "" >> "$report_file"
    echo "## Generated Artifacts" >> "$report_file"
    echo "" >> "$report_file"
    echo "The following files contain detailed test data:" >> "$report_file"
    echo "" >> "$report_file"
    
    # List all generated files
    find "$HEALTH_PATH" -name "*test*" -o -name "*benchmark*" -o -name "*report*" | grep -E "\.(json|txt|md)$" | while read -r file; do
        echo "- \`$(basename "$file")\` - $(stat -f%Sm -t '%Y-%m-%d %H:%M' "$file" 2>/dev/null || stat -c %y "$file" | cut -d' ' -f1-2)" >> "$report_file"
    done
    
    # Add recommendations
    cat >> "$report_file" << EOF

## Recommendations

### Immediate Actions
- ğŸ” Review any failed tests and address root causes
- ğŸ“Š Establish baseline performance metrics from benchmark results
- ğŸš¨ Set up monitoring for production deployment

### Optimization Opportunities
- âš¡ Optimize parallel execution efficiency if below 3x sequential performance
- ğŸ”„ Implement auto-scaling based on load test patterns
- ğŸ“ˆ Add performance regression testing to CI/CD pipeline

### Production Readiness
- ğŸ›¡ï¸ Verify error handling and recovery mechanisms
- ğŸ“Š Implement real-time performance monitoring
- ğŸ”„ Set up automated health checks based on test patterns

## Conclusion

This comprehensive test suite validates the multi-agent workflow system's readiness for production deployment. Review the detailed results and address any identified issues before proceeding with production rollout.

**Report Generated:** $(date)
**Total Test Duration:** $(( $(date +%s) - $(stat -f%Sm -t %s "$LOG_FILE" 2>/dev/null || stat -c %Y "$LOG_FILE") )) seconds
EOF

    log "${GREEN}âœ… Comprehensive report generated: $report_file${NC}"
}

# Function to cleanup test artifacts
cleanup_test_artifacts() {
    log "\n${BLUE}ğŸ§¹ Cleaning up test artifacts...${NC}"
    
    # Clean up temporary test files
    find "$HEALTH_PATH" -name "test-state-*.json" -delete 2>/dev/null || true
    find "$HEALTH_PATH" -name "benchmark-state-*.json" -delete 2>/dev/null || true
    find "$HEALTH_PATH" -name "*.tmp" -delete 2>/dev/null || true
    
    # Archive old test results (keep last 10)
    if ls "$HEALTH_PATH"/*test-report* >/dev/null 2>&1; then
        ls -t "$HEALTH_PATH"/*test-report* | tail -n +11 | xargs rm -f 2>/dev/null || true
    fi
    
    log "${GREEN}âœ… Cleanup completed${NC}"
}

# Main execution function
main() {
    local start_time=$(date +%s)
    local test_failures=0
    
    log "${GREEN}ğŸš€ Starting Comprehensive Multi-Agent Workflow Integration Tests${NC}"
    log "=================================================================="
    log "ğŸ“… Start Time: $(date)"
    log "ğŸ“ Base Path: $BASE_PATH"
    log "ğŸ“ Log File: $LOG_FILE"
    log ""
    
    # Check prerequisites
    check_prerequisites || exit 1
    
    # Run component validation tests
    log "${CYAN}ğŸ”§ Phase 1: Component Validation${NC}"
    validate_agent_system || ((test_failures++))
    test_workflow_commands || ((test_failures++))
    test_hook_system || ((test_failures++))
    
    # Run integration tests
    log "\n${CYAN}ğŸ§ª Phase 2: Integration Testing${NC}"
    run_integration_tests || ((test_failures++))
    
    # Run performance benchmarks
    log "\n${CYAN}ğŸ“Š Phase 3: Performance Benchmarking${NC}"
    run_performance_benchmarks || ((test_failures++))
    
    # Run load tests (optional)
    log "\n${CYAN}ğŸ”¥ Phase 4: Load Testing${NC}"
    load_test_result=$(run_load_tests)
    load_test_exit=$?
    if [[ $load_test_exit -eq 1 ]]; then
        ((test_failures++))
    elif [[ $load_test_exit -eq 2 ]]; then
        log "${YELLOW}âš ï¸  Load tests were skipped${NC}"
    fi
    
    # Generate comprehensive report
    generate_comprehensive_report
    
    # Cleanup
    cleanup_test_artifacts
    
    # Final summary
    local end_time=$(date +%s)
    local total_duration=$((end_time - start_time))
    
    log "\n${GREEN}ğŸ Test Suite Execution Complete${NC}"
    log "=================================================================="
    log "ğŸ“… End Time: $(date)"
    log "â±ï¸  Total Duration: ${total_duration}s ($(($total_duration / 60))m $(($total_duration % 60))s)"
    log "ğŸ“Š Test Failures: $test_failures"
    
    if [[ $test_failures -eq 0 ]]; then
        log "${GREEN}ğŸ‰ ALL TESTS PASSED - System ready for production deployment!${NC}"
        exit 0
    else
        log "${RED}âš ï¸  $test_failures TEST CATEGORIES FAILED - Review results before production deployment${NC}"
        exit 1
    fi
}

# Handle script interruption
trap 'log "\nğŸ›‘ Test execution interrupted"; cleanup_test_artifacts; exit 130' INT TERM

# Execute main function
main "$@"