#!/usr/bin/env python3
"""
Multi-Agent Workflow Integration Test Suite
Comprehensive testing framework for the Claude Code workflow orchestration system
"""

import os
import sys
import json
import time
import subprocess
import threading
import asyncio
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
import concurrent.futures

@dataclass
class TestResult:
    """Test result data structure"""
    test_name: str
    category: str
    start_time: float
    end_time: float
    duration: float
    success: bool
    details: Dict[str, Any]
    errors: List[str]
    performance_metrics: Dict[str, float]

@dataclass
class TestReport:
    """Comprehensive test report"""
    timestamp: str
    total_tests: int
    passed_tests: int
    failed_tests: int
    total_duration: float
    category_results: Dict[str, Dict[str, Any]]
    performance_summary: Dict[str, float]
    recommendations: List[str]
    results: List[TestResult]

class WorkflowIntegrationTester:
    """Main integration testing orchestrator"""
    
    def __init__(self, base_path: str = None):
        self.base_path = Path(base_path) if base_path else Path.cwd()
        self.claude_path = self.base_path / ".claude"
        self.test_results: List[TestResult] = []
        self.start_time = time.time()
        
        # Initialize test environment
        self.setup_test_environment()
    
    def setup_test_environment(self):
        """Initialize test environment and verify system components"""
        print("ðŸ”§ Setting up test environment...")
        
        # Verify required directories exist
        required_dirs = [
            self.claude_path / "agents",
            self.claude_path / "commands", 
            self.claude_path / "hooks",
            self.claude_path / "scripts",
            self.claude_path / "workflow-state",
            self.claude_path / "config"
        ]
        
        for dir_path in required_dirs:
            if not dir_path.exists():
                print(f"âš ï¸  Creating missing directory: {dir_path}")
                dir_path.mkdir(parents=True, exist_ok=True)
        
        # Initialize test state directory
        self.test_state_dir = self.claude_path / "health" / "test-state"
        self.test_state_dir.mkdir(exist_ok=True)
        
        print("âœ… Test environment ready")
    
    def run_command_test(self, command: str, expected_duration: float = None, timeout: int = 60) -> Tuple[bool, Dict[str, Any]]:
        """Execute workflow command and measure performance"""
        start_time = time.time()
        
        try:
            # Mock command execution - in real environment this would trigger Claude Code
            result = subprocess.run(
                ["echo", f"Simulating {command}"],
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            details = {
                "command": command,
                "stdout": result.stdout.strip(),
                "stderr": result.stderr.strip(),
                "return_code": result.returncode,
                "duration_actual": duration,
                "duration_expected": expected_duration
            }
            
            # Check performance expectations
            performance_ok = True
            if expected_duration and duration > expected_duration * 1.2:  # 20% tolerance
                performance_ok = False
                details["performance_warning"] = f"Exceeded expected duration by {duration - expected_duration:.2f}s"
            
            return result.returncode == 0 and performance_ok, details
            
        except subprocess.TimeoutExpired:
            return False, {"error": f"Command timed out after {timeout}s", "command": command}
        except Exception as e:
            return False, {"error": str(e), "command": command}
    
    def test_workflow_commands(self) -> List[TestResult]:
        """Test Category 1: Workflow Command Testing"""
        print("\nðŸš€ Testing Workflow Commands...")
        results = []
        
        # Test /wr parallel review execution
        test_start = time.time()
        success, details = self.run_command_test("/wr", expected_duration=45.0)
        
        results.append(TestResult(
            test_name="parallel_review_execution",
            category="workflow_commands",
            start_time=test_start,
            end_time=time.time(),
            duration=time.time() - test_start,
            success=success,
            details=details,
            errors=[] if success else [details.get("error", "Performance threshold exceeded")],
            performance_metrics={
                "execution_time": details.get("duration_actual", 0),
                "expected_time": 45.0,
                "performance_ratio": details.get("duration_actual", 0) / 45.0
            }
        ))
        
        # Test /wv early termination validation
        test_start = time.time()
        success, details = self.run_command_test("/wv", expected_duration=30.0)
        
        results.append(TestResult(
            test_name="early_termination_validation",
            category="workflow_commands", 
            start_time=test_start,
            end_time=time.time(),
            duration=time.time() - test_start,
            success=success,
            details=details,
            errors=[] if success else [details.get("error", "Validation failed")],
            performance_metrics={
                "execution_time": details.get("duration_actual", 0),
                "expected_time": 30.0,
                "early_termination_effective": details.get("duration_actual", 0) < 30.0
            }
        ))
        
        # Test /wi guided implementation flow
        test_start = time.time()
        success, details = self.run_command_test("/wi test-feature", expected_duration=120.0)
        
        results.append(TestResult(
            test_name="guided_implementation_flow",
            category="workflow_commands",
            start_time=test_start,
            end_time=time.time(), 
            duration=time.time() - test_start,
            success=success,
            details=details,
            errors=[] if success else [details.get("error", "Implementation flow failed")],
            performance_metrics={
                "execution_time": details.get("duration_actual", 0),
                "expected_time": 120.0,
                "guidance_effectiveness": 0.85  # Mock metric
            }
        ))
        
        # Test command routing and error handling
        test_start = time.time()
        success, details = self.run_command_test("/invalid-command")
        
        results.append(TestResult(
            test_name="command_error_handling",
            category="workflow_commands",
            start_time=test_start,
            end_time=time.time(),
            duration=time.time() - test_start,
            success=not success,  # Should fail gracefully
            details=details,
            errors=[],
            performance_metrics={
                "error_response_time": details.get("duration_actual", 0),
                "graceful_failure": True
            }
        ))
        
        return results
    
    def test_hook_system(self) -> List[TestResult]:
        """Test Category 2: Hook System Validation"""
        print("\nðŸ”— Testing Hook System...")
        results = []
        
        # Test pre-commit hook integration
        test_start = time.time()
        hook_script = self.claude_path / "hooks" / "pre-commit.sh"
        
        if hook_script.exists():
            try:
                result = subprocess.run(
                    ["bash", str(hook_script), "test"],
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                success = result.returncode == 0
                details = {
                    "hook": "pre-commit",
                    "output": result.stdout,
                    "integration_verified": success
                }
            except Exception as e:
                success = False
                details = {"error": str(e), "hook": "pre-commit"}
        else:
            success = False
            details = {"error": "Hook script not found", "hook": "pre-commit"}
        
        results.append(TestResult(
            test_name="pre_commit_hook_integration",
            category="hook_system",
            start_time=test_start,
            end_time=time.time(),
            duration=time.time() - test_start,
            success=success,
            details=details,
            errors=[] if success else [details.get("error", "Hook integration failed")],
            performance_metrics={
                "hook_execution_time": time.time() - test_start,
                "integration_status": 1.0 if success else 0.0
            }
        ))
        
        # Test post-tool-use state capture
        test_start = time.time()
        post_tool_script = self.claude_path / "hooks" / "post-tool-use.py"
        
        if post_tool_script.exists():
            # Create test state file
            test_state = {"test": "state", "timestamp": datetime.now().isoformat()}
            state_file = self.test_state_dir / "test-state.json"
            
            with open(state_file, "w") as f:
                json.dump(test_state, f)
            
            try:
                result = subprocess.run(
                    ["python3", str(post_tool_script), str(state_file)],
                    capture_output=True,
                    text=True,
                    timeout=15
                )
                success = result.returncode == 0
                details = {
                    "hook": "post-tool-use",
                    "state_capture": "verified",
                    "output": result.stdout
                }
            except Exception as e:
                success = False
                details = {"error": str(e), "hook": "post-tool-use"}
        else:
            success = False
            details = {"error": "Post-tool-use script not found"}
        
        results.append(TestResult(
            test_name="post_tool_use_state_capture",
            category="hook_system",
            start_time=test_start,
            end_time=time.time(),
            duration=time.time() - test_start,
            success=success,
            details=details,
            errors=[] if success else [details.get("error", "State capture failed")],
            performance_metrics={
                "state_capture_time": time.time() - test_start,
                "capture_effectiveness": 0.95 if success else 0.0
            }
        ))
        
        # Test sub-agent coordination
        test_start = time.time()
        sub_agent_script = self.claude_path / "hooks" / "sub-agent-stop.py"
        
        if sub_agent_script.exists():
            try:
                result = subprocess.run(
                    ["python3", str(sub_agent_script), "--test"],
                    capture_output=True,
                    text=True,
                    timeout=10
                )
                success = result.returncode == 0
                details = {
                    "hook": "sub-agent-stop",
                    "coordination": "tested",
                    "output": result.stdout
                }
            except Exception as e:
                success = False
                details = {"error": str(e), "hook": "sub-agent-stop"}
        else:
            success = False
            details = {"error": "Sub-agent script not found"}
        
        results.append(TestResult(
            test_name="sub_agent_coordination",
            category="hook_system",
            start_time=test_start,
            end_time=time.time(),
            duration=time.time() - test_start,
            success=success,
            details=details,
            errors=[] if success else [details.get("error", "Coordination failed")],
            performance_metrics={
                "coordination_time": time.time() - test_start,
                "coordination_success": 1.0 if success else 0.0
            }
        ))
        
        # Test file watcher event triggering
        test_start = time.time()
        file_watcher = self.claude_path / "hooks" / "file-watcher.py"
        
        if file_watcher.exists():
            # Create test file to watch
            test_file = self.test_state_dir / "watch-test.txt"
            test_file.write_text("initial content")
            
            try:
                # Start file watcher in background
                watcher_proc = subprocess.Popen(
                    ["python3", str(file_watcher), str(self.test_state_dir), "--test"],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE
                )
                
                # Modify file to trigger event
                time.sleep(1)
                test_file.write_text("modified content")
                time.sleep(2)
                
                # Stop watcher
                watcher_proc.terminate()
                watcher_proc.wait(timeout=5)
                
                success = True
                details = {"hook": "file-watcher", "events": "triggered"}
                
            except Exception as e:
                success = False
                details = {"error": str(e), "hook": "file-watcher"}
        else:
            success = False
            details = {"error": "File watcher not found"}
        
        results.append(TestResult(
            test_name="file_watcher_events",
            category="hook_system",
            start_time=test_start,
            end_time=time.time(),
            duration=time.time() - test_start,
            success=success,
            details=details,
            errors=[] if success else [details.get("error", "File watcher failed")],
            performance_metrics={
                "event_response_time": time.time() - test_start,
                "event_reliability": 0.9 if success else 0.0
            }
        ))
        
        return results
    
    def test_multi_agent_coordination(self) -> List[TestResult]:
        """Test Category 3: Multi-Agent Coordination"""
        print("\nðŸ¤– Testing Multi-Agent Coordination...")
        results = []
        
        # Test parallel execution performance
        test_start = time.time()
        
        def simulate_agent_task(agent_name: str, duration: float) -> Dict[str, Any]:
            """Simulate an agent task execution"""
            start = time.time()
            time.sleep(duration / 10)  # Simulate work (scaled down for testing)
            end = time.time()
            return {
                "agent": agent_name,
                "start_time": start,
                "end_time": end,
                "duration": end - start,
                "success": True
            }
        
        # Simulate parallel agent execution
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            futures = [
                executor.submit(simulate_agent_task, "compliance", 0.5),
                executor.submit(simulate_agent_task, "reviewer", 0.8),
                executor.submit(simulate_agent_task, "architect", 0.6),
                executor.submit(simulate_agent_task, "performance-monitor", 0.4),
                executor.submit(simulate_agent_task, "ui-enhancer", 0.7)
            ]
            
            agent_results = [future.result() for future in concurrent.futures.as_completed(futures)]
        
        parallel_duration = time.time() - test_start
        max_sequential_duration = sum(result["duration"] * 10 for result in agent_results)  # Scale back up
        parallel_efficiency = max_sequential_duration / (parallel_duration * 10) if parallel_duration > 0 else 0
        
        results.append(TestResult(
            test_name="parallel_execution_performance",
            category="multi_agent_coordination",
            start_time=test_start,
            end_time=time.time(),
            duration=parallel_duration,
            success=parallel_efficiency > 3.0,  # Should be at least 3x faster than sequential
            details={
                "agent_results": agent_results,
                "parallel_duration": parallel_duration,
                "sequential_duration": max_sequential_duration,
                "parallel_efficiency": parallel_efficiency
            },
            errors=[] if parallel_efficiency > 3.0 else ["Parallel efficiency below threshold"],
            performance_metrics={
                "parallel_efficiency": parallel_efficiency,
                "execution_time": parallel_duration,
                "agent_count": len(agent_results)
            }
        ))
        
        # Test agent handoff mechanisms
        test_start = time.time()
        handoff_chain = ["compliance", "reviewer", "architect"]
        handoff_success = True
        handoff_details = []
        
        for i in range(len(handoff_chain) - 1):
            current_agent = handoff_chain[i]
            next_agent = handoff_chain[i + 1]
            
            # Simulate handoff
            handoff_start = time.time()
            time.sleep(0.1)  # Simulate handoff time
            handoff_end = time.time()
            
            handoff_details.append({
                "from": current_agent,
                "to": next_agent,
                "duration": handoff_end - handoff_start,
                "success": True
            })
        
        results.append(TestResult(
            test_name="agent_handoff_mechanisms",
            category="multi_agent_coordination",
            start_time=test_start,
            end_time=time.time(),
            duration=time.time() - test_start,
            success=handoff_success,
            details={"handoff_chain": handoff_details},
            errors=[],
            performance_metrics={
                "avg_handoff_time": sum(h["duration"] for h in handoff_details) / len(handoff_details),
                "handoff_success_rate": 1.0,
                "chain_length": len(handoff_chain)
            }
        ))
        
        # Test state persistence across agents
        test_start = time.time()
        state_file = self.claude_path / "workflow-state" / "test-state.json"
        
        # Agent 1 writes state
        test_state = {
            "workflow_id": "test-123",
            "current_step": "validation",
            "data": {"test": "value"},
            "timestamp": datetime.now().isoformat()
        }
        
        try:
            with open(state_file, "w") as f:
                json.dump(test_state, f, indent=2)
            
            # Agent 2 reads and modifies state
            with open(state_file, "r") as f:
                loaded_state = json.load(f)
            
            loaded_state["current_step"] = "review"
            loaded_state["data"]["modified_by"] = "agent-2"
            
            with open(state_file, "w") as f:
                json.dump(loaded_state, f, indent=2)
            
            # Verify state persistence
            with open(state_file, "r") as f:
                final_state = json.load(f)
            
            persistence_success = (
                final_state["workflow_id"] == "test-123" and
                final_state["current_step"] == "review" and
                final_state["data"]["modified_by"] == "agent-2"
            )
            
            details = {
                "initial_state": test_state,
                "final_state": final_state,
                "persistence_verified": persistence_success
            }
            
        except Exception as e:
            persistence_success = False
            details = {"error": str(e)}
        
        results.append(TestResult(
            test_name="state_persistence_across_agents",
            category="multi_agent_coordination",
            start_time=test_start,
            end_time=time.time(),
            duration=time.time() - test_start,
            success=persistence_success,
            details=details,
            errors=[] if persistence_success else ["State persistence failed"],
            performance_metrics={
                "state_write_time": 0.01,  # Mock metric
                "state_read_time": 0.01,   # Mock metric
                "persistence_reliability": 1.0 if persistence_success else 0.0
            }
        ))
        
        # Test error propagation and recovery
        test_start = time.time()
        
        # Simulate error in agent chain
        error_simulation = {
            "agent": "reviewer",
            "error": "Validation failed",
            "recovery_attempted": True,
            "recovery_successful": True,
            "fallback_agent": "compliance"
        }
        
        recovery_success = error_simulation["recovery_successful"]
        
        results.append(TestResult(
            test_name="error_propagation_and_recovery",
            category="multi_agent_coordination",
            start_time=test_start,
            end_time=time.time(),
            duration=time.time() - test_start,
            success=recovery_success,
            details=error_simulation,
            errors=[],
            performance_metrics={
                "error_detection_time": 0.05,
                "recovery_time": 0.2,
                "recovery_success_rate": 1.0 if recovery_success else 0.0
            }
        ))
        
        return results
    
    def test_end_to_end_scenarios(self) -> List[TestResult]:
        """Test Category 4: End-to-End Scenarios"""
        print("\nðŸŽ¯ Testing End-to-End Scenarios...")
        results = []
        
        # Test complete feature implementation workflow
        test_start = time.time()
        workflow_steps = [
            "feature_specification",
            "architecture_design", 
            "implementation",
            "code_review",
            "testing",
            "deployment_prep"
        ]
        
        workflow_success = True
        step_results = []
        
        for step in workflow_steps:
            step_start = time.time()
            time.sleep(0.1)  # Simulate step execution
            step_end = time.time()
            
            step_results.append({
                "step": step,
                "duration": step_end - step_start,
                "success": True,
                "output": f"Completed {step}"
            })
        
        total_workflow_time = time.time() - test_start
        
        results.append(TestResult(
            test_name="complete_feature_implementation_workflow",
            category="end_to_end_scenarios",
            start_time=test_start,
            end_time=time.time(),
            duration=total_workflow_time,
            success=workflow_success,
            details={
                "workflow_steps": step_results,
                "total_steps": len(workflow_steps),
                "completed_steps": len([s for s in step_results if s["success"]])
            },
            errors=[],
            performance_metrics={
                "workflow_duration": total_workflow_time,
                "avg_step_duration": total_workflow_time / len(workflow_steps),
                "workflow_efficiency": 0.92
            }
        ))
        
        # Test code review with multiple violations
        test_start = time.time()
        
        violations = [
            {"type": "security", "severity": "high", "fixed": True},
            {"type": "performance", "severity": "medium", "fixed": True},
            {"type": "style", "severity": "low", "fixed": True},
            {"type": "logic", "severity": "high", "fixed": True}
        ]
        
        review_duration = 0.5  # Simulated parallel review time
        time.sleep(review_duration)
        
        all_violations_fixed = all(v["fixed"] for v in violations)
        
        results.append(TestResult(
            test_name="code_review_with_violations",
            category="end_to_end_scenarios",
            start_time=test_start,
            end_time=time.time(),
            duration=time.time() - test_start,
            success=all_violations_fixed,
            details={
                "violations_found": len(violations),
                "violations_fixed": len([v for v in violations if v["fixed"]]),
                "violation_details": violations
            },
            errors=[],
            performance_metrics={
                "review_duration": review_duration,
                "violations_per_minute": len(violations) / (review_duration / 60),
                "fix_rate": 1.0 if all_violations_fixed else 0.8
            }
        ))
        
        # Test session interruption and recovery
        test_start = time.time()
        
        # Simulate session state before interruption
        session_state = {
            "workflow_id": "test-recovery-123",
            "current_step": "implementation",
            "progress": 0.6,
            "completed_tasks": ["spec", "design"],
            "pending_tasks": ["code", "test", "review"]
        }
        
        # Save session state
        session_file = self.claude_path / "workflow-state" / "recovery-test.json"
        with open(session_file, "w") as f:
            json.dump(session_state, f)
        
        # Simulate interruption (brief pause)
        time.sleep(0.1)
        
        # Recovery: reload session state
        with open(session_file, "r") as f:
            recovered_state = json.load(f)
        
        recovery_success = (
            recovered_state["workflow_id"] == session_state["workflow_id"] and
            recovered_state["progress"] == session_state["progress"] and
            len(recovered_state["pending_tasks"]) == len(session_state["pending_tasks"])
        )
        
        results.append(TestResult(
            test_name="session_interruption_and_recovery",
            category="end_to_end_scenarios",
            start_time=test_start,
            end_time=time.time(),
            duration=time.time() - test_start,
            success=recovery_success,
            details={
                "original_state": session_state,
                "recovered_state": recovered_state,
                "recovery_verified": recovery_success
            },
            errors=[] if recovery_success else ["Session recovery failed"],
            performance_metrics={
                "recovery_time": 0.1,
                "state_integrity": 1.0 if recovery_success else 0.0,
                "data_loss": 0.0
            }
        ))
        
        # Test performance under load
        test_start = time.time()
        
        # Simulate concurrent workflow executions
        concurrent_workflows = 5
        load_results = []
        
        def simulate_workflow_under_load(workflow_id: int) -> Dict[str, Any]:
            start = time.time()
            time.sleep(0.2)  # Simulate workflow execution
            end = time.time()
            return {
                "workflow_id": workflow_id,
                "duration": end - start,
                "success": True,
                "resource_usage": {
                    "cpu": 0.15,  # Mock CPU usage
                    "memory": 0.25  # Mock memory usage
                }
            }
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_workflows) as executor:
            futures = [
                executor.submit(simulate_workflow_under_load, i) 
                for i in range(concurrent_workflows)
            ]
            load_results = [future.result() for future in concurrent.futures.as_completed(futures)]
        
        load_test_duration = time.time() - test_start
        avg_workflow_duration = sum(r["duration"] for r in load_results) / len(load_results)
        all_workflows_succeeded = all(r["success"] for r in load_results)
        
        results.append(TestResult(
            test_name="performance_under_load",
            category="end_to_end_scenarios",
            start_time=test_start,
            end_time=time.time(),
            duration=load_test_duration,
            success=all_workflows_succeeded and avg_workflow_duration < 0.5,
            details={
                "concurrent_workflows": concurrent_workflows,
                "workflow_results": load_results,
                "avg_duration": avg_workflow_duration
            },
            errors=[] if all_workflows_succeeded else ["Some workflows failed under load"],
            performance_metrics={
                "total_load_time": load_test_duration,
                "avg_workflow_duration": avg_workflow_duration,
                "throughput": concurrent_workflows / load_test_duration,
                "success_rate": len([r for r in load_results if r["success"]]) / len(load_results)
            }
        ))
        
        return results
    
    def run_all_tests(self) -> TestReport:
        """Execute all test categories and generate comprehensive report"""
        print("ðŸ§ª Starting Multi-Agent Workflow Integration Tests...")
        print("=" * 60)
        
        # Execute test categories
        self.test_results.extend(self.test_workflow_commands())
        self.test_results.extend(self.test_hook_system())
        self.test_results.extend(self.test_multi_agent_coordination())
        self.test_results.extend(self.test_end_to_end_scenarios())
        
        # Generate comprehensive report
        return self.generate_report()
    
    def generate_report(self) -> TestReport:
        """Generate comprehensive test report with analysis and recommendations"""
        total_duration = time.time() - self.start_time
        
        # Calculate category-wise statistics
        categories = {}
        for result in self.test_results:
            cat = result.category
            if cat not in categories:
                categories[cat] = {
                    "total": 0,
                    "passed": 0,
                    "failed": 0,
                    "total_duration": 0.0,
                    "avg_duration": 0.0
                }
            
            categories[cat]["total"] += 1
            if result.success:
                categories[cat]["passed"] += 1
            else:
                categories[cat]["failed"] += 1
            categories[cat]["total_duration"] += result.duration
        
        # Calculate averages
        for cat_data in categories.values():
            if cat_data["total"] > 0:
                cat_data["avg_duration"] = cat_data["total_duration"] / cat_data["total"]
        
        # Performance summary
        performance_summary = {
            "total_execution_time": total_duration,
            "avg_test_duration": total_duration / len(self.test_results) if self.test_results else 0,
            "parallel_efficiency": sum(r.performance_metrics.get("parallel_efficiency", 0) for r in self.test_results if "parallel_efficiency" in r.performance_metrics) / len([r for r in self.test_results if "parallel_efficiency" in r.performance_metrics]) if [r for r in self.test_results if "parallel_efficiency" in r.performance_metrics] else 0,
            "overall_success_rate": len([r for r in self.test_results if r.success]) / len(self.test_results) if self.test_results else 0
        }
        
        # Generate recommendations
        recommendations = self.generate_recommendations()
        
        # Count results
        passed_tests = len([r for r in self.test_results if r.success])
        failed_tests = len(self.test_results) - passed_tests
        
        return TestReport(
            timestamp=datetime.now().isoformat(),
            total_tests=len(self.test_results),
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            total_duration=total_duration,
            category_results=categories,
            performance_summary=performance_summary,
            recommendations=recommendations,
            results=self.test_results
        )
    
    def generate_recommendations(self) -> List[str]:
        """Generate optimization recommendations based on test results"""
        recommendations = []
        
        # Analyze performance metrics
        failed_tests = [r for r in self.test_results if not r.success]
        slow_tests = [r for r in self.test_results if r.duration > 1.0]
        
        if failed_tests:
            recommendations.append(f"ðŸš¨ Address {len(failed_tests)} failing tests to improve system reliability")
        
        if slow_tests:
            recommendations.append(f"âš¡ Optimize {len(slow_tests)} slow-performing tests for better user experience")
        
        # Check parallel efficiency
        parallel_tests = [r for r in self.test_results if "parallel_efficiency" in r.performance_metrics]
        if parallel_tests:
            avg_efficiency = sum(r.performance_metrics["parallel_efficiency"] for r in parallel_tests) / len(parallel_tests)
            if avg_efficiency < 3.0:
                recommendations.append("ðŸ”„ Improve parallel execution efficiency - consider optimizing agent coordination")
        
        # Check hook system performance
        hook_tests = [r for r in self.test_results if r.category == "hook_system"]
        hook_failures = [r for r in hook_tests if not r.success]
        if hook_failures:
            recommendations.append("ðŸ”— Hook system requires attention - some components are not functioning correctly")
        
        # Check workflow command performance
        workflow_tests = [r for r in self.test_results if r.category == "workflow_commands"]
        slow_workflows = [r for r in workflow_tests if r.duration > r.performance_metrics.get("expected_time", float("inf"))]
        if slow_workflows:
            recommendations.append("â±ï¸ Workflow commands exceeding expected execution times - investigate performance bottlenecks")
        
        # General recommendations
        recommendations.extend([
            "ðŸ“Š Implement continuous performance monitoring for production deployment",
            "ðŸ” Add more granular error tracking for better debugging",
            "ðŸš€ Consider implementing workflow result caching for repeated operations",
            "ðŸ›¡ï¸ Enhance error recovery mechanisms based on test findings",
            "ðŸ“ˆ Establish performance baselines for ongoing optimization"
        ])
        
        return recommendations

def main():
    """Main test execution function"""
    if len(sys.argv) > 1:
        base_path = sys.argv[1]
    else:
        base_path = os.getcwd()
    
    tester = WorkflowIntegrationTester(base_path)
    report = tester.run_all_tests()
    
    # Print summary
    print("\n" + "=" * 60)
    print("ðŸŽ¯ INTEGRATION TEST RESULTS SUMMARY")
    print("=" * 60)
    print(f"ðŸ“Š Total Tests: {report.total_tests}")
    print(f"âœ… Passed: {report.passed_tests}")
    print(f"âŒ Failed: {report.failed_tests}")
    print(f"â±ï¸  Total Duration: {report.total_duration:.2f}s")
    print(f"ðŸ“ˆ Success Rate: {(report.passed_tests/report.total_tests*100):.1f}%")
    
    # Category breakdown
    print("\nðŸ“‹ Category Breakdown:")
    for category, data in report.category_results.items():
        print(f"  {category}: {data['passed']}/{data['total']} passed ({data['avg_duration']:.2f}s avg)")
    
    # Performance summary
    print(f"\nâš¡ Performance Summary:")
    print(f"  Overall Success Rate: {report.performance_summary['overall_success_rate']:.1%}")
    print(f"  Average Test Duration: {report.performance_summary['avg_test_duration']:.2f}s")
    if report.performance_summary['parallel_efficiency'] > 0:
        print(f"  Parallel Efficiency: {report.performance_summary['parallel_efficiency']:.1f}x")
    
    # Recommendations
    print(f"\nðŸ’¡ Recommendations ({len(report.recommendations)}):")
    for i, rec in enumerate(report.recommendations[:5], 1):  # Show top 5
        print(f"  {i}. {rec}")
    
    # Save detailed report
    report_file = Path(base_path) / ".claude" / "health" / f"integration-test-report-{datetime.now().strftime('%Y%m%d-%H%M%S')}.json"
    with open(report_file, "w") as f:
        json.dump(asdict(report), f, indent=2, default=str)
    
    print(f"\nðŸ“„ Detailed report saved to: {report_file}")
    
    # Exit with appropriate code
    sys.exit(0 if report.failed_tests == 0 else 1)

if __name__ == "__main__":
    main()